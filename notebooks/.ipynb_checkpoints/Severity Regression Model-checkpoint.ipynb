{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.style\n",
    "matplotlib.style.use(\"seaborn\")\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import pickle\n",
    "\n",
    "import seaborn as sns\n",
    "sns.color_palette(\"hls\", 17)\n",
    "import scipy.stats as st\n",
    "import math\n",
    "from tqdm.notebook import tqdm\n",
    "from pingouin import rcorr\n",
    "import pingouin as pg\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor,RandomForestRegressor, AdaBoostRegressor\n",
    "from sklearn.multioutput import RegressorChain, MultiOutputRegressor\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifierCV\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn import linear_model\n",
    "from sklearn import svm \n",
    "from sklearn.model_selection import KFold, StratifiedKFold, train_test_split, cross_validate, cross_val_score, learning_curve\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, f1_score, precision_score, recall_score, plot_roc_curve, roc_curve, auc, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import feature_selection #import chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.join(\"..\"))\n",
    "\n",
    "from module.trend_analysis_functions import *\n",
    "from module.topic_model_plus_class import Topic_Model_plus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(os.path.join(os.path.dirname(os.getcwd()),'data','ICS_predictive_sitreps_test.csv')).drop([\"Unnamed: 0\"], axis=1)\n",
    "train_data = pd.read_csv(os.path.join(os.path.dirname(os.getcwd()),'data','ICS_predictive_sitreps_train.csv')).drop([\"Unnamed: 0\"], axis=1)\n",
    "val_data = pd.read_csv(os.path.join(os.path.dirname(os.getcwd()),'data','ICS_predictive_sitreps_val.csv')).drop([\"Unnamed: 0\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CY</th>\n",
       "      <th>DISCOVERY_DATE</th>\n",
       "      <th>INCIDENT_ID</th>\n",
       "      <th>PCT_CONTAINED_COMPLETED</th>\n",
       "      <th>START_YEAR</th>\n",
       "      <th>TOTAL_AERIAL</th>\n",
       "      <th>TOTAL_PERSONNEL</th>\n",
       "      <th>REPORT_DOY</th>\n",
       "      <th>DISCOVERY_DOY</th>\n",
       "      <th>Combined_Text</th>\n",
       "      <th>...</th>\n",
       "      <th>INC_MGMT_ORG_ABBREV_2</th>\n",
       "      <th>INC_MGMT_ORG_ABBREV_3</th>\n",
       "      <th>INC_MGMT_ORG_ABBREV_4</th>\n",
       "      <th>INC_MGMT_ORG_ABBREV_5</th>\n",
       "      <th>INC_MGMT_ORG_ABBREV_A</th>\n",
       "      <th>INC_MGMT_ORG_ABBREV_B</th>\n",
       "      <th>INC_MGMT_ORG_ABBREV_C</th>\n",
       "      <th>INC_MGMT_ORG_ABBREV_D</th>\n",
       "      <th>INC_MGMT_ORG_ABBREV_E</th>\n",
       "      <th>INC_MGMT_ORG_ABBREV_F</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010</td>\n",
       "      <td>2010-07-15 15:00:00</td>\n",
       "      <td>2000_CA-RRU-062485_VALLEY COMPLEX</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>0.119048</td>\n",
       "      <td>0.123789</td>\n",
       "      <td>197</td>\n",
       "      <td>196</td>\n",
       "      <td>['resource', 'share', 'cactus']</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010</td>\n",
       "      <td>2010-07-15 15:00:00</td>\n",
       "      <td>2000_CA-RRU-062485_VALLEY COMPLEX</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>0.119048</td>\n",
       "      <td>0.123789</td>\n",
       "      <td>197</td>\n",
       "      <td>196</td>\n",
       "      <td>['resource', 'share', 'incident', 'cactus', 'i...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010</td>\n",
       "      <td>2010-07-15 15:00:00</td>\n",
       "      <td>2000_CA-RRU-062485_VALLEY COMPLEX</td>\n",
       "      <td>0.000007</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>0.095238</td>\n",
       "      <td>0.088805</td>\n",
       "      <td>197</td>\n",
       "      <td>196</td>\n",
       "      <td>['resource', 'share', 'cactus', 'erratic', 'wi...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010</td>\n",
       "      <td>2010-07-15 15:00:00</td>\n",
       "      <td>2000_CA-RRU-062485_VALLEY COMPLEX</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>0.103175</td>\n",
       "      <td>0.103516</td>\n",
       "      <td>197</td>\n",
       "      <td>196</td>\n",
       "      <td>['resource', 'share', 'cactus', 'cactus', 'bec...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010</td>\n",
       "      <td>2010-07-15 15:00:00</td>\n",
       "      <td>2000_CA-RRU-062485_VALLEY COMPLEX</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>2010.0</td>\n",
       "      <td>0.103175</td>\n",
       "      <td>0.103516</td>\n",
       "      <td>197</td>\n",
       "      <td>196</td>\n",
       "      <td>['resource', 'share', 'cactus', 'cactus', 'bec...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18574</th>\n",
       "      <td>2014</td>\n",
       "      <td>2014-03-15 14:30:00</td>\n",
       "      <td>2014_VAVAS1403037_BEAVER LODGE RD.</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006997</td>\n",
       "      <td>74</td>\n",
       "      <td>74</td>\n",
       "      <td>['fast', 'spread', 'field']</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18575</th>\n",
       "      <td>2014</td>\n",
       "      <td>2014-03-19 14:00:00</td>\n",
       "      <td>2014_VAVAS1406037_AIRPORT MOUNTAIN</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009957</td>\n",
       "      <td>80</td>\n",
       "      <td>78</td>\n",
       "      <td>['heavy', 'plume', 'primary', 'carrier']</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18576</th>\n",
       "      <td>2014</td>\n",
       "      <td>2014-08-20 13:00:00</td>\n",
       "      <td>2014_WA-WFS-513_SAND RIDGE</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.051130</td>\n",
       "      <td>234</td>\n",
       "      <td>232</td>\n",
       "      <td>['heavy', 'canyon', 'river', 'mainly', 'canyon...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18577</th>\n",
       "      <td>2014</td>\n",
       "      <td>2014-08-20 13:00:00</td>\n",
       "      <td>2014_WA-WFS-513_SAND RIDGE</td>\n",
       "      <td>0.000022</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>0.023810</td>\n",
       "      <td>0.064586</td>\n",
       "      <td>235</td>\n",
       "      <td>232</td>\n",
       "      <td>['laid', 'night', 'test', 'wind', 'remain', 'c...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18578</th>\n",
       "      <td>2014</td>\n",
       "      <td>2014-08-20 13:00:00</td>\n",
       "      <td>2014_WA-WFS-513_SAND RIDGE</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>2014.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.024758</td>\n",
       "      <td>235</td>\n",
       "      <td>232</td>\n",
       "      <td>['report', 'incident', 'wind', 'test', 'overni...</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18579 rows × 80 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         CY       DISCOVERY_DATE                         INCIDENT_ID  \\\n",
       "0      2010  2010-07-15 15:00:00   2000_CA-RRU-062485_VALLEY COMPLEX   \n",
       "1      2010  2010-07-15 15:00:00   2000_CA-RRU-062485_VALLEY COMPLEX   \n",
       "2      2010  2010-07-15 15:00:00   2000_CA-RRU-062485_VALLEY COMPLEX   \n",
       "3      2010  2010-07-15 15:00:00   2000_CA-RRU-062485_VALLEY COMPLEX   \n",
       "4      2010  2010-07-15 15:00:00   2000_CA-RRU-062485_VALLEY COMPLEX   \n",
       "...     ...                  ...                                 ...   \n",
       "18574  2014  2014-03-15 14:30:00  2014_VAVAS1403037_BEAVER LODGE RD.   \n",
       "18575  2014  2014-03-19 14:00:00  2014_VAVAS1406037_AIRPORT MOUNTAIN   \n",
       "18576  2014  2014-08-20 13:00:00          2014_WA-WFS-513_SAND RIDGE   \n",
       "18577  2014  2014-08-20 13:00:00          2014_WA-WFS-513_SAND RIDGE   \n",
       "18578  2014  2014-08-20 13:00:00          2014_WA-WFS-513_SAND RIDGE   \n",
       "\n",
       "       PCT_CONTAINED_COMPLETED  START_YEAR  TOTAL_AERIAL  TOTAL_PERSONNEL  \\\n",
       "0                     0.000020      2010.0      0.119048         0.123789   \n",
       "1                     0.000015      2010.0      0.119048         0.123789   \n",
       "2                     0.000007      2010.0      0.095238         0.088805   \n",
       "3                     0.000025      2010.0      0.103175         0.103516   \n",
       "4                     0.000015      2010.0      0.103175         0.103516   \n",
       "...                        ...         ...           ...              ...   \n",
       "18574                 0.000025      2014.0      0.000000         0.006997   \n",
       "18575                 0.000021      2014.0      0.000000         0.009957   \n",
       "18576                 0.000000      2014.0      0.023810         0.051130   \n",
       "18577                 0.000022      2014.0      0.023810         0.064586   \n",
       "18578                 0.000025      2014.0      0.000000         0.024758   \n",
       "\n",
       "       REPORT_DOY  DISCOVERY_DOY  \\\n",
       "0             197            196   \n",
       "1             197            196   \n",
       "2             197            196   \n",
       "3             197            196   \n",
       "4             197            196   \n",
       "...           ...            ...   \n",
       "18574          74             74   \n",
       "18575          80             78   \n",
       "18576         234            232   \n",
       "18577         235            232   \n",
       "18578         235            232   \n",
       "\n",
       "                                           Combined_Text  ...  \\\n",
       "0                        ['resource', 'share', 'cactus']  ...   \n",
       "1      ['resource', 'share', 'incident', 'cactus', 'i...  ...   \n",
       "2      ['resource', 'share', 'cactus', 'erratic', 'wi...  ...   \n",
       "3      ['resource', 'share', 'cactus', 'cactus', 'bec...  ...   \n",
       "4      ['resource', 'share', 'cactus', 'cactus', 'bec...  ...   \n",
       "...                                                  ...  ...   \n",
       "18574                        ['fast', 'spread', 'field']  ...   \n",
       "18575           ['heavy', 'plume', 'primary', 'carrier']  ...   \n",
       "18576  ['heavy', 'canyon', 'river', 'mainly', 'canyon...  ...   \n",
       "18577  ['laid', 'night', 'test', 'wind', 'remain', 'c...  ...   \n",
       "18578  ['report', 'incident', 'wind', 'test', 'overni...  ...   \n",
       "\n",
       "      INC_MGMT_ORG_ABBREV_2  INC_MGMT_ORG_ABBREV_3  INC_MGMT_ORG_ABBREV_4  \\\n",
       "0                         0                      0                      1   \n",
       "1                         0                      0                      1   \n",
       "2                         0                      0                      1   \n",
       "3                         0                      0                      1   \n",
       "4                         0                      0                      1   \n",
       "...                     ...                    ...                    ...   \n",
       "18574                     0                      0                      0   \n",
       "18575                     0                      0                      0   \n",
       "18576                     0                      0                      0   \n",
       "18577                     0                      0                      0   \n",
       "18578                     0                      0                      0   \n",
       "\n",
       "       INC_MGMT_ORG_ABBREV_5  INC_MGMT_ORG_ABBREV_A  INC_MGMT_ORG_ABBREV_B  \\\n",
       "0                          0                      0                      0   \n",
       "1                          0                      0                      0   \n",
       "2                          0                      0                      0   \n",
       "3                          0                      0                      0   \n",
       "4                          0                      0                      0   \n",
       "...                      ...                    ...                    ...   \n",
       "18574                      1                      0                      0   \n",
       "18575                      1                      0                      0   \n",
       "18576                      1                      0                      0   \n",
       "18577                      1                      0                      0   \n",
       "18578                      1                      0                      0   \n",
       "\n",
       "       INC_MGMT_ORG_ABBREV_C  INC_MGMT_ORG_ABBREV_D  INC_MGMT_ORG_ABBREV_E  \\\n",
       "0                          0                      0                      0   \n",
       "1                          0                      0                      0   \n",
       "2                          0                      0                      0   \n",
       "3                          0                      0                      0   \n",
       "4                          0                      0                      0   \n",
       "...                      ...                    ...                    ...   \n",
       "18574                      0                      0                      0   \n",
       "18575                      0                      0                      0   \n",
       "18576                      0                      0                      0   \n",
       "18577                      0                      0                      0   \n",
       "18578                      0                      0                      0   \n",
       "\n",
       "      INC_MGMT_ORG_ABBREV_F  \n",
       "0                         0  \n",
       "1                         0  \n",
       "2                         0  \n",
       "3                         0  \n",
       "4                         0  \n",
       "...                     ...  \n",
       "18574                     0  \n",
       "18575                     0  \n",
       "18576                     0  \n",
       "18577                     0  \n",
       "18578                     0  \n",
       "\n",
       "[18579 rows x 80 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CY', 'DISCOVERY_DATE', 'INCIDENT_ID', 'PCT_CONTAINED_COMPLETED',\n",
       "       'START_YEAR', 'TOTAL_AERIAL', 'TOTAL_PERSONNEL', 'REPORT_DOY',\n",
       "       'DISCOVERY_DOY', 'Combined_Text', 'Unique_IDs', 'ACRES', 'WF_FSR',\n",
       "       'INJURIES', 'FATALITIES', 'EST_IM_COST_TO_DATE', 'STR_DAMAGED',\n",
       "       'STR_DESTROYED', 'NEW_ACRES', 'POO_STATE', 'POO_LATITUDE',\n",
       "       'POO_LONGITUDE', 'WEATHER_CONCERNS_NARR', 'INC_MGMT_ORG_ABBREV',\n",
       "       'EVACUATION_IN_PROGRESS', 'Incident_region', 'NUM_REPORTS',\n",
       "       'DAYS_BURING', 'Severity', 'Traffic', 'Command_Transitions',\n",
       "       'Evacuations', 'Inaccurate_Mapping', 'Aerial_Grounding',\n",
       "       'Resource_Issues', 'Injuries', 'Cultural_Resources', 'Livestock',\n",
       "       'Law_Violations', 'Military_Base', 'Infrastructure', 'Extreme_Weather',\n",
       "       'Ecological', 'Hazardous_Terrain', 'Floods', 'Dry_Weather',\n",
       "       'Total_Incident_Text', 'Current_total_Injuries',\n",
       "       'Current_total_Structures_Damages',\n",
       "       'Current_total_Structures_Destroyed', 'Current_total_Fatalities',\n",
       "       'Diff_Injuries', 'Diff_Structures_Damages', 'Diff_Structures_Destroyed',\n",
       "       'Diff_Fatalities', 'Total_Injuries', 'Total_Structures_Damages',\n",
       "       'Total_Structures_Destroyed', 'Total_Fatalities',\n",
       "       'Incident_region_AICC', 'Incident_region_CA', 'Incident_region_EACC',\n",
       "       'Incident_region_GBCC', 'Incident_region_HICC', 'Incident_region_NRCC',\n",
       "       'Incident_region_NWCC', 'Incident_region_RMCC', 'Incident_region_SACC',\n",
       "       'Incident_region_SWCC', 'INC_MGMT_ORG_ABBREV_1',\n",
       "       'INC_MGMT_ORG_ABBREV_2', 'INC_MGMT_ORG_ABBREV_3',\n",
       "       'INC_MGMT_ORG_ABBREV_4', 'INC_MGMT_ORG_ABBREV_5',\n",
       "       'INC_MGMT_ORG_ABBREV_A', 'INC_MGMT_ORG_ABBREV_B',\n",
       "       'INC_MGMT_ORG_ABBREV_C', 'INC_MGMT_ORG_ABBREV_D',\n",
       "       'INC_MGMT_ORG_ABBREV_E', 'INC_MGMT_ORG_ABBREV_F'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "xcols = ['TOTAL_AERIAL', 'TOTAL_PERSONNEL', 'WF_FSR', 'DAYS_BURING', 'ACRES','PCT_CONTAINED_COMPLETED', 'Current_total_Injuries',\n",
    "       'Current_total_Structures_Damages',\n",
    "       'Current_total_Structures_Destroyed', 'Current_total_Fatalities','Traffic',\n",
    "       'Command_Transitions', 'Evacuations', 'Inaccurate_Mapping',\n",
    "       'Aerial_Grounding', 'Resource_Issues', 'Injuries', 'Cultural_Resources',\n",
    "       'Livestock', 'Law_Violations', 'Military_Base', 'Infrastructure',\n",
    "       'Extreme_Weather', 'Ecological', 'Hazardous_Terrain', 'Floods',\n",
    "       'Dry_Weather', 'Incident_region_AICC', 'Incident_region_CA', 'Incident_region_EACC',\n",
    "       'Incident_region_GBCC', 'Incident_region_HICC', 'Incident_region_NRCC',\n",
    "       'Incident_region_NWCC', 'Incident_region_RMCC', 'Incident_region_SACC',\n",
    "       'Incident_region_SWCC']\n",
    "ycols = ['Diff_Injuries', 'Diff_Structures_Damages', 'Diff_Structures_Destroyed',\n",
    "       'Diff_Fatalities']\n",
    "Xtrain = train_data[xcols]; ytrain = train_data[ycols]\n",
    "Xval = val_data[xcols]; yval = val_data[ycols]\n",
    "Xtest = test_data[xcols]; ytest = test_data[ycols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18579, 37)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xtrain.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2245, 37)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xval.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models to test:\n",
    "- SVM\n",
    "- NB\n",
    "- Logistic Regression\n",
    "- Random Forest\n",
    "- xgboost\n",
    "- AdaBoost\n",
    "- NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>train score</th>\n",
       "      <th>test score</th>\n",
       "      <th>train MSE</th>\n",
       "      <th>test MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SVM</td>\n",
       "      <td>-13332.455</td>\n",
       "      <td>-3.954</td>\n",
       "      <td>8.182</td>\n",
       "      <td>0.874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>GradientBoosting</td>\n",
       "      <td>0.140</td>\n",
       "      <td>-0.507</td>\n",
       "      <td>2.895</td>\n",
       "      <td>2.646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>-59.305</td>\n",
       "      <td>-1.196</td>\n",
       "      <td>6.813</td>\n",
       "      <td>0.870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bayesian Ridge</td>\n",
       "      <td>-0.299</td>\n",
       "      <td>-0.240</td>\n",
       "      <td>8.177</td>\n",
       "      <td>0.750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MLP</td>\n",
       "      <td>-1169.337</td>\n",
       "      <td>-0.084</td>\n",
       "      <td>7.807</td>\n",
       "      <td>1.105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>-15750.500</td>\n",
       "      <td>-0.232</td>\n",
       "      <td>8.177</td>\n",
       "      <td>0.749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AdaBoost</td>\n",
       "      <td>0.946</td>\n",
       "      <td>-2.330</td>\n",
       "      <td>0.147</td>\n",
       "      <td>0.877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>1.000</td>\n",
       "      <td>-0.220</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.173</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              model  train score  test score  train MSE  test MSE\n",
       "0               SVM   -13332.455      -3.954      8.182     0.874\n",
       "1  GradientBoosting        0.140      -0.507      2.895     2.646\n",
       "2     Random Forest      -59.305      -1.196      6.813     0.870\n",
       "3    Bayesian Ridge       -0.299      -0.240      8.177     0.750\n",
       "4               MLP    -1169.337      -0.084      7.807     1.105\n",
       "5           XGBoost   -15750.500      -0.232      8.177     0.749\n",
       "6          AdaBoost        0.946      -2.330      0.147     0.877\n",
       "7     Decision Tree        1.000      -0.220      0.000     1.173"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_names = [\"SVM\", \"GradientBoosting\", \"Random Forest\", \"Bayesian Ridge\", \"MLP\", \"XGBoost\", \"AdaBoost\", \"Decision Tree\"]\n",
    "train_score = []; test_score = []; train_MSE = []; test_MSE = []\n",
    "models = [svm.SVR(kernel='rbf', C=0.2), GradientBoostingRegressor(max_features='log2',n_estimators=100, random_state=0, max_depth=5), \n",
    "          RandomForestRegressor(criterion='mae',max_features='sqrt',n_estimators=100, random_state=0, max_depth=20),\n",
    "          linear_model.BayesianRidge(alpha_1=10000, alpha_2=100, lambda_1=1, lambda_2=0.0001),\n",
    "          MLPRegressor(random_state=0, max_iter=1000), xgb.XGBRegressor(booster='gblinear',n_estimators=100),\n",
    "         AdaBoostRegressor(base_estimator=DecisionTreeRegressor(max_features='auto', splitter='random'), n_estimators=450, learning_rate=1, loss='square'), \n",
    "          DecisionTreeRegressor(criterion='mae', max_features='log2', splitter='random')]\n",
    "for m in models:\n",
    "    mdl = MultiOutputRegressor(m)\n",
    "    mdl.fit(Xtrain, ytrain)\n",
    "    train_pred = np.around(mdl.predict(Xtrain)); train_pred[train_pred<0] = 0\n",
    "    test_pred = np.around(mdl.predict(Xtest)); test_pred[test_pred<0] = 0\n",
    "    train_score.append(round(r2_score(train_pred, ytrain),3))\n",
    "    test_score.append(round(r2_score(test_pred, ytest),3))\n",
    "    train_MSE.append(round(mean_squared_error(ytrain, train_pred),3))\n",
    "    test_MSE.append(round(mean_squared_error(ytest, test_pred),3))\n",
    "\n",
    "results_df = pd.DataFrame({\"model\": model_names,\n",
    "                          \"train score\": train_score,\n",
    "                          \"test score\": test_score,\n",
    "                          \"train MSE\": train_MSE,\n",
    "                          \"test MSE\": test_MSE})\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = xgb.XGBRegressor(booster='gblinear',n_estimators=100)\n",
    "mdl = MultiOutputRegressor(m)\n",
    "mdl.fit(Xtrain, ytrain)\n",
    "preds = np.around(mdl.predict(Xtest))\n",
    "preds[preds<0]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = pd.DataFrame(preds, columns = ytest.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  1.,  2.,  3.,  4., 15., 13., 14.,  6.,  5.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ytest['Diff_Injuries'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 2., 3., 4., 5.], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds['Diff_Injuries'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.path.join(os.path.dirname(os.getcwd()),'models','severity_model_test.sav')\n",
    "pickle.dump(mdl, open(filename, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper parameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rbf\n",
      "poly\n",
      "sigmoid\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kernels</th>\n",
       "      <th>train r</th>\n",
       "      <th>test r</th>\n",
       "      <th>train MSE</th>\n",
       "      <th>test MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rbf</td>\n",
       "      <td>0.341</td>\n",
       "      <td>-0.121</td>\n",
       "      <td>8.047</td>\n",
       "      <td>0.847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>poly</td>\n",
       "      <td>0.350</td>\n",
       "      <td>-0.912</td>\n",
       "      <td>8.016</td>\n",
       "      <td>0.943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sigmoid</td>\n",
       "      <td>-1696.329</td>\n",
       "      <td>-1045632.155</td>\n",
       "      <td>11578.157</td>\n",
       "      <td>12558.180</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   kernels   train r       test r  train MSE   test MSE\n",
       "0      rbf     0.341       -0.121      8.047      0.847\n",
       "1     poly     0.350       -0.912      8.016      0.943\n",
       "2  sigmoid -1696.329 -1045632.155  11578.157  12558.180"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernals = ['rbf', 'poly', 'sigmoid']\n",
    "train_r = []\n",
    "test_r = []\n",
    "train_MSE = []\n",
    "test_MSE = []\n",
    "for kernal in kernals:\n",
    "    print(kernal)\n",
    "    mdl = MultiOutputRegressor(svm.SVR(kernel=kernal))\n",
    "    \n",
    "    mdl.fit(Xtrain, ytrain)\n",
    "    train_r.append(round(mdl.score(Xtrain, ytrain),3))\n",
    "    test_r.append(round(mdl.score(Xtest, ytest),3))\n",
    "    train_MSE.append(round(mean_squared_error(ytrain, mdl.predict(Xtrain)),3))\n",
    "    test_MSE.append(round(mean_squared_error(ytest, mdl.predict(Xtest)),3))\n",
    "\n",
    "model_comparisons = pd.DataFrame({\n",
    "    \"kernels\": kernals,\n",
    "    \"train r\": train_r,\n",
    "    \"test r\": test_r,\n",
    "    \"train MSE\": train_MSE,\n",
    "    \"test MSE\": test_MSE})\n",
    "\n",
    "model_comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>c-value</th>\n",
       "      <th>train r</th>\n",
       "      <th>test r</th>\n",
       "      <th>train MSE</th>\n",
       "      <th>test MSE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.1</td>\n",
       "      <td>0.288</td>\n",
       "      <td>-0.080</td>\n",
       "      <td>8.220</td>\n",
       "      <td>0.896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.2</td>\n",
       "      <td>0.304</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>8.166</td>\n",
       "      <td>0.874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.3</td>\n",
       "      <td>0.314</td>\n",
       "      <td>-0.003</td>\n",
       "      <td>8.132</td>\n",
       "      <td>0.863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.4</td>\n",
       "      <td>0.322</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>8.106</td>\n",
       "      <td>0.857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.328</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>8.087</td>\n",
       "      <td>0.855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.6</td>\n",
       "      <td>0.332</td>\n",
       "      <td>-0.058</td>\n",
       "      <td>8.074</td>\n",
       "      <td>0.850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.7</td>\n",
       "      <td>0.335</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>8.064</td>\n",
       "      <td>0.849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.8</td>\n",
       "      <td>0.338</td>\n",
       "      <td>-0.096</td>\n",
       "      <td>8.057</td>\n",
       "      <td>0.846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.9</td>\n",
       "      <td>0.339</td>\n",
       "      <td>-0.113</td>\n",
       "      <td>8.052</td>\n",
       "      <td>0.847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.341</td>\n",
       "      <td>-0.121</td>\n",
       "      <td>8.047</td>\n",
       "      <td>0.847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.1</td>\n",
       "      <td>0.342</td>\n",
       "      <td>-0.127</td>\n",
       "      <td>8.043</td>\n",
       "      <td>0.847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.2</td>\n",
       "      <td>0.343</td>\n",
       "      <td>-0.133</td>\n",
       "      <td>8.039</td>\n",
       "      <td>0.847</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    c-value  train r  test r  train MSE  test MSE\n",
       "0       0.1    0.288  -0.080      8.220     0.896\n",
       "1       0.2    0.304  -0.001      8.166     0.874\n",
       "2       0.3    0.314  -0.003      8.132     0.863\n",
       "3       0.4    0.322  -0.019      8.106     0.857\n",
       "4       0.5    0.328  -0.040      8.087     0.855\n",
       "5       0.6    0.332  -0.058      8.074     0.850\n",
       "6       0.7    0.335  -0.078      8.064     0.849\n",
       "7       0.8    0.338  -0.096      8.057     0.846\n",
       "8       0.9    0.339  -0.113      8.052     0.847\n",
       "9       1.0    0.341  -0.121      8.047     0.847\n",
       "10      1.1    0.342  -0.127      8.043     0.847\n",
       "11      1.2    0.343  -0.133      8.039     0.847"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_vals = [1e-1,2e-1,3e-1,4e-1,5e-1, 6e-1,7e-1,8e-1, 9e-1, 1, 1.1, 1.2]\n",
    "train_r = []\n",
    "test_r = []\n",
    "train_MSE = []\n",
    "test_MSE = []\n",
    "for c in c_vals:\n",
    "    mdl = MultiOutputRegressor(svm.SVR(C=c,kernel='rbf'))\n",
    "    \n",
    "    mdl.fit(Xtrain, ytrain)\n",
    "    train_r.append(round(mdl.score(Xtrain, ytrain),3))\n",
    "    test_r.append(round(mdl.score(Xtest, ytest),3))\n",
    "    train_MSE.append(round(mean_squared_error(ytrain, mdl.predict(Xtrain)),3))\n",
    "    test_MSE.append(round(mean_squared_error(ytest, mdl.predict(Xtest)),3))\n",
    "\n",
    "model_comparisons = pd.DataFrame({\n",
    "    \"c-value\": c_vals,\n",
    "    \"train r\": train_r,\n",
    "    \"test r\": test_r,\n",
    "    \"train MSE\": train_MSE,\n",
    "    \"test MSE\": test_MSE})\n",
    "\n",
    "model_comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosting regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'loss' : ['huber'.'squared_error', 'absolute_error', 'huber', 'quantile']}#, #'lad', #'ls'\n",
    "#               'criterion': ['friedman_mse', 'mse'],\n",
    "#               'max_depth': [i for i in range(3,25)],\n",
    "#               'n_estimators': [i for i in range(100, 500, 25)],\n",
    "#               'max_features': ['auto', 'sqrt', 'log2']}\n",
    "best_params = {'criterion': 'friedman_mse', 'max_depth': 3, 'n_estimators': 100, 'max_features': 'auto'}#}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/5 [00:00<?, ?it/s]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:07<00:00,  7.18s/it]\u001b[A\n",
      " 20%|████████████████▊                                                                   | 1/5 [00:07<00:28,  7.20s/it]\n",
      "  0%|                                                                                            | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████████████████████████                                          | 1/2 [00:07<00:07,  7.14s/it]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:14<00:00,  7.14s/it]\u001b[A\n",
      " 40%|█████████████████████████████████▌                                                  | 2/5 [00:21<00:27,  9.32s/it]\n",
      "  0%|                                                                                           | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "  5%|███▊                                                                               | 1/22 [00:07<02:28,  7.08s/it]\u001b[A\n",
      "  9%|███████▌                                                                           | 2/22 [00:16<02:33,  7.66s/it]\u001b[A\n",
      " 14%|███████████▎                                                                       | 3/22 [00:27<02:45,  8.72s/it]\u001b[A\n",
      " 18%|███████████████                                                                    | 4/22 [00:41<03:05, 10.31s/it]\u001b[A\n",
      " 23%|██████████████████▊                                                                | 5/22 [00:58<03:30, 12.40s/it]\u001b[A\n",
      " 27%|██████████████████████▋                                                            | 6/22 [01:20<04:05, 15.37s/it]\u001b[A\n",
      " 32%|██████████████████████████▍                                                        | 7/22 [01:49<04:48, 19.24s/it]\u001b[A\n",
      " 36%|██████████████████████████████▏                                                    | 8/22 [02:24<05:37, 24.10s/it]\u001b[A\n",
      " 41%|█████████████████████████████████▉                                                 | 9/22 [03:11<06:43, 31.01s/it]\u001b[A\n",
      " 45%|█████████████████████████████████████▎                                            | 10/22 [04:11<07:56, 39.74s/it]\u001b[A\n",
      " 50%|█████████████████████████████████████████                                         | 11/22 [05:29<09:22, 51.17s/it]\u001b[A\n",
      " 55%|████████████████████████████████████████████▋                                     | 12/22 [07:12<11:05, 66.58s/it]\u001b[A\n",
      " 59%|████████████████████████████████████████████████▍                                 | 13/22 [09:16<12:36, 84.03s/it]\u001b[A\n",
      " 64%|███████████████████████████████████████████████████▌                             | 14/22 [12:04<14:32, 109.05s/it]\u001b[A\n",
      " 68%|███████████████████████████████████████████████████████▏                         | 15/22 [15:19<15:44, 134.92s/it]\u001b[A\n",
      " 73%|██████████████████████████████████████████████████████████▉                      | 16/22 [19:01<16:06, 161.11s/it]\u001b[A\n",
      " 77%|██████████████████████████████████████████████████████████████▌                  | 17/22 [23:31<16:07, 193.57s/it]\u001b[A\n",
      " 82%|██████████████████████████████████████████████████████████████████▎              | 18/22 [28:48<15:22, 230.67s/it]\u001b[A\n",
      " 86%|█████████████████████████████████████████████████████████████████████▉           | 19/22 [34:42<13:23, 267.76s/it]\u001b[A\n",
      " 91%|█████████████████████████████████████████████████████████████████████████▋       | 20/22 [41:12<10:08, 304.21s/it]\u001b[A\n",
      " 95%|█████████████████████████████████████████████████████████████████████████████▎   | 21/22 [47:31<05:26, 326.80s/it]\u001b[A\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 22/22 [54:21<00:00, 148.25s/it]\u001b[A\n",
      " 60%|█████████████████████████████████████████████████▊                                 | 3/5 [54:42<32:49, 984.95s/it]\n",
      "  0%|                                                                                           | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|█████▏                                                                             | 1/16 [00:07<01:53,  7.58s/it]\u001b[A\n",
      " 12%|██████████▍                                                                        | 2/16 [00:17<01:54,  8.18s/it]\u001b[A\n",
      " 19%|███████████████▌                                                                   | 3/16 [00:28<01:58,  9.14s/it]\u001b[A\n",
      " 25%|████████████████████▊                                                              | 4/16 [00:41<02:04, 10.40s/it]\u001b[A\n",
      " 31%|█████████████████████████▉                                                         | 5/16 [00:56<02:09, 11.80s/it]\u001b[A\n",
      " 38%|███████████████████████████████▏                                                   | 6/16 [01:13<02:13, 13.31s/it]\u001b[A\n",
      " 44%|████████████████████████████████████▎                                              | 7/16 [01:32<02:15, 15.05s/it]\u001b[A\n",
      " 50%|█████████████████████████████████████████▌                                         | 8/16 [01:53<02:14, 16.77s/it]\u001b[A\n",
      " 56%|██████████████████████████████████████████████▋                                    | 9/16 [02:16<02:09, 18.48s/it]\u001b[A\n",
      " 62%|███████████████████████████████████████████████████▎                              | 10/16 [02:40<02:01, 20.23s/it]\u001b[A\n",
      " 69%|████████████████████████████████████████████████████████▍                         | 11/16 [03:06<01:50, 22.02s/it]\u001b[A\n",
      " 75%|█████████████████████████████████████████████████████████████▌                    | 12/16 [03:34<01:35, 23.86s/it]\u001b[A\n",
      " 81%|██████████████████████████████████████████████████████████████████▋               | 13/16 [04:04<01:17, 25.76s/it]\u001b[A\n",
      " 88%|███████████████████████████████████████████████████████████████████████▊          | 14/16 [04:36<00:55, 27.52s/it]\u001b[A\n",
      " 94%|████████████████████████████████████████████████████████████████████████████▉     | 15/16 [05:10<00:29, 29.41s/it]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 16/16 [05:46<00:00, 21.63s/it]\u001b[A\n",
      " 80%|████████████████████████████████████████████████████████████████▊                | 4/5 [1:00:28<13:13, 793.27s/it]\n",
      "  0%|                                                                                            | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      " 33%|████████████████████████████                                                        | 1/3 [00:07<00:15,  7.58s/it]\u001b[A\n",
      " 67%|████████████████████████████████████████████████████████                            | 2/3 [00:11<00:06,  6.47s/it]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:15<00:00,  5.17s/it]\u001b[A\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████| 5/5 [1:00:44<00:00, 728.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 'huber', 'criterion': 'friedman_mse', 'max_depth': 3, 'n_estimators': 100, 'max_features': 'auto'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for param in tqdm(params):\n",
    "    train_r = []\n",
    "    test_r = []\n",
    "    train_MSE = []\n",
    "    test_MSE = []\n",
    "    for val in tqdm(params[param]):\n",
    "        test_params = {param:val}\n",
    "        test_params.update(best_params)\n",
    "        clf = MultiOutputRegressor(GradientBoostingRegressor(**test_params))\n",
    "        clf.fit(Xtrain, ytrain)\n",
    "        train_r.append(round(clf.score(Xtrain, ytrain),3))\n",
    "        test_r.append(round(clf.score(Xtest, ytest),3))\n",
    "        train_MSE.append(round(mean_squared_error(ytrain, clf.predict(Xtrain)),3))\n",
    "        test_MSE.append(round(mean_squared_error(ytest, clf.predict(Xtest)),3))\n",
    "    best_params[param] = params[param][np.argmin(test_MSE)]\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient_boosting_mdl = GradientBoostingRegressor(max_features='auto',n_estimators=100, random_state=0, max_depth=3,criterion='friedman_mse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'criterion': ['mse', 'mae'],#'absolute_error', \n",
    "                            #'squared_error'],\n",
    "              'max_depth': [i for i in range(3,25)],\n",
    "              'n_estimators': [i for i in range(100, 500, 25)],\n",
    "              'max_features': ['auto', 'sqrt', 'log2']}\n",
    "best_params={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/4 [00:00<?, ?it/s]\n",
      "  0%|                                                                                            | 0/1 [00:00<?, ?it/s]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 1/1 [00:27<00:00, 27.05s/it]\u001b[A\n",
      " 25%|█████████████████████                                                               | 1/4 [00:27<01:21, 27.05s/it]\n",
      "  0%|                                                                                           | 0/22 [00:00<?, ?it/s]\u001b[A\n",
      "  5%|███▊                                                                               | 1/22 [00:11<03:57, 11.33s/it]\u001b[A\n",
      "  9%|███████▌                                                                           | 2/22 [00:24<04:00, 12.01s/it]\u001b[A\n",
      " 14%|███████████▎                                                                       | 3/22 [00:40<04:08, 13.08s/it]\u001b[A\n",
      " 18%|███████████████                                                                    | 4/22 [00:57<04:14, 14.12s/it]\u001b[A\n",
      " 23%|██████████████████▊                                                                | 5/22 [01:14<04:19, 15.24s/it]\u001b[A\n",
      " 27%|██████████████████████▋                                                            | 6/22 [01:33<04:19, 16.24s/it]\u001b[A\n",
      " 32%|██████████████████████████▍                                                        | 7/22 [01:53<04:18, 17.26s/it]\u001b[A\n",
      " 36%|██████████████████████████████▏                                                    | 8/22 [02:13<04:14, 18.20s/it]\u001b[A\n",
      " 41%|█████████████████████████████████▉                                                 | 9/22 [02:35<04:12, 19.44s/it]\u001b[A\n",
      " 45%|█████████████████████████████████████▎                                            | 10/22 [02:58<04:04, 20.40s/it]\u001b[A\n",
      " 50%|█████████████████████████████████████████                                         | 11/22 [03:21<03:53, 21.27s/it]\u001b[A\n",
      " 55%|████████████████████████████████████████████▋                                     | 12/22 [03:46<03:41, 22.18s/it]\u001b[A\n",
      " 59%|████████████████████████████████████████████████▍                                 | 13/22 [04:10<03:26, 22.97s/it]\u001b[A\n",
      " 64%|████████████████████████████████████████████████████▏                             | 14/22 [04:36<03:09, 23.70s/it]\u001b[A\n",
      " 68%|███████████████████████████████████████████████████████▉                          | 15/22 [05:02<02:50, 24.39s/it]\u001b[A\n",
      " 73%|███████████████████████████████████████████████████████████▋                      | 16/22 [05:28<02:30, 25.01s/it]\u001b[A\n",
      " 77%|███████████████████████████████████████████████████████████████▎                  | 17/22 [05:55<02:07, 25.51s/it]\u001b[A\n",
      " 82%|███████████████████████████████████████████████████████████████████               | 18/22 [06:22<01:43, 25.86s/it]\u001b[A\n",
      " 86%|██████████████████████████████████████████████████████████████████████▊           | 19/22 [06:49<01:18, 26.25s/it]\u001b[A\n",
      " 91%|██████████████████████████████████████████████████████████████████████████▌       | 20/22 [07:16<00:53, 26.54s/it]\u001b[A\n",
      " 95%|██████████████████████████████████████████████████████████████████████████████▎   | 21/22 [07:43<00:26, 26.79s/it]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 22/22 [08:11<00:00, 22.33s/it]\u001b[A\n",
      " 50%|█████████████████████████████████████████▌                                         | 2/4 [08:38<05:32, 166.31s/it]\n",
      "  0%|                                                                                           | 0/16 [00:00<?, ?it/s]\u001b[A\n",
      "  6%|█████▏                                                                             | 1/16 [00:27<06:45, 27.05s/it]\u001b[A\n",
      " 12%|██████████▍                                                                        | 2/16 [01:01<06:48, 29.16s/it]\u001b[A\n",
      " 19%|███████████████▌                                                                   | 3/16 [01:45<07:16, 33.58s/it]\u001b[A\n",
      " 25%|████████████████████▊                                                              | 4/16 [02:35<07:42, 38.56s/it]\u001b[A\n",
      " 31%|█████████████████████████▉                                                         | 5/16 [03:32<08:05, 44.09s/it]\u001b[A\n",
      " 38%|███████████████████████████████▏                                                   | 6/16 [04:39<08:29, 50.94s/it]\u001b[A\n",
      " 44%|████████████████████████████████████▎                                              | 7/16 [05:53<08:40, 57.83s/it]\u001b[A\n",
      " 50%|█████████████████████████████████████████▌                                         | 8/16 [07:17<08:47, 65.94s/it]\u001b[A\n",
      " 56%|██████████████████████████████████████████████▋                                    | 9/16 [09:12<09:23, 80.50s/it]\u001b[A\n",
      " 62%|██████████████████████████████████████████████████▋                              | 10/16 [11:47<10:17, 102.92s/it]\u001b[A\n",
      " 69%|███████████████████████████████████████████████████████▋                         | 11/16 [14:32<10:08, 121.65s/it]\u001b[A\n",
      " 75%|████████████████████████████████████████████████████████████▊                    | 12/16 [17:30<09:13, 138.40s/it]\u001b[A\n",
      " 81%|█████████████████████████████████████████████████████████████████▊               | 13/16 [20:41<07:42, 154.11s/it]\u001b[A\n",
      " 88%|██████████████████████████████████████████████████████████████████████▉          | 14/16 [22:38<04:46, 143.03s/it]\u001b[A\n",
      " 94%|███████████████████████████████████████████████████████████████████████████▉     | 15/16 [24:30<02:13, 133.75s/it]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 16/16 [26:28<00:00, 99.28s/it]\u001b[A\n",
      " 75%|██████████████████████████████████████████████████████████████▎                    | 3/4 [35:06<09:52, 592.98s/it]\n",
      "  0%|                                                                                            | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      " 33%|████████████████████████████                                                        | 1/3 [00:24<00:49, 24.56s/it]\u001b[A\n",
      " 67%|████████████████████████████████████████████████████████                            | 2/3 [00:32<00:19, 19.51s/it]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:39<00:00, 13.21s/it]\u001b[A\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 4/4 [35:46<00:00, 536.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'mse', 'max_depth': 20, 'n_estimators': 100, 'max_features': 'sqrt'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for param in tqdm(params):\n",
    "    train_r = []\n",
    "    test_r = []\n",
    "    train_MSE = []\n",
    "    test_MSE = []\n",
    "    for val in tqdm(params[param]):\n",
    "        test_params = {param:val}\n",
    "        test_params.update(best_params)\n",
    "        clf = MultiOutputRegressor(RandomForestRegressor(**test_params))\n",
    "        clf.fit(Xtrain, ytrain)\n",
    "        train_r.append(round(clf.score(Xtrain, ytrain),3))\n",
    "        test_r.append(round(clf.score(Xtest, ytest),3))\n",
    "        train_MSE.append(round(mean_squared_error(ytrain, clf.predict(Xtrain)),3))\n",
    "        test_MSE.append(round(mean_squared_error(ytest, clf.predict(Xtest)),3))\n",
    "    best_params[param] = params[param][np.argmin(test_MSE)]\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor(criterion='mae',max_features='sqrt',n_estimators=100, random_state=0, max_depth=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Ridge "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [00:03<00:00,  3.85it/s]\n",
      "  0%|                                                                                           | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha_1 best value: 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [00:03<00:00,  3.82it/s]\n",
      "  0%|                                                                                           | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alpha_2 best value: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [00:03<00:00,  3.94it/s]\n",
      "  0%|                                                                                           | 0/15 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda_1 best value: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 15/15 [00:03<00:00,  3.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda_2 best value: 0.0001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "vals = [10000, 100,1000,10,1,0.01,1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7, 1e-8, 1e-9, 1e-10]\n",
    "params = ['alpha_1','alpha_2','lambda_1','lambda_2']\n",
    "for param in params:\n",
    "    train_r = []\n",
    "    test_r = []\n",
    "    train_MSE = []\n",
    "    test_MSE = []\n",
    "    for val in tqdm(vals):\n",
    "        input_params = {param:val}\n",
    "        clf = MultiOutputRegressor(linear_model.BayesianRidge(**input_params))\n",
    "        clf.fit(Xtrain, ytrain)\n",
    "        train_r.append(round(clf.score(Xtrain, ytrain),3))\n",
    "        test_r.append(round(clf.score(Xtest, ytest),3))\n",
    "        train_MSE.append(round(mean_squared_error(ytrain, clf.predict(Xtrain)),3))\n",
    "        test_MSE.append(round(mean_squared_error(ytest, clf.predict(Xtest)),3))\n",
    "    print(param+\" best value:\", vals[np.argmin(test_MSE)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge = linear_model.BayesianRidge(alpha_1=10000, alpha_2=100, lambda_1=1, lambda_2=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/3 [00:00<?, ?it/s]C:\\Users\\srandrad\\Anaconda3\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\srandrad\\Anaconda3\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\srandrad\\Anaconda3\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\srandrad\\Anaconda3\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      " 67%|████████████████████████████████████████████████████████                            | 2/3 [00:08<00:04,  4.66s/it]C:\\Users\\srandrad\\Anaconda3\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\srandrad\\Anaconda3\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\srandrad\\Anaconda3\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\srandrad\\Anaconda3\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 3/3 [00:51<00:00, 17.25s/it]\n",
      "  0%|                                                                                           | 0/16 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "booster best value: gblinear\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\srandrad\\Anaconda3\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\srandrad\\Anaconda3\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\srandrad\\Anaconda3\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\srandrad\\Anaconda3\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "  6%|█████▏                                                                             | 1/16 [00:05<01:24,  5.63s/it]C:\\Users\\srandrad\\Anaconda3\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\srandrad\\Anaconda3\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\srandrad\\Anaconda3\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\srandrad\\Anaconda3\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      " 12%|██████████▍                                                                        | 2/16 [00:12<01:24,  6.01s/it]C:\\Users\\srandrad\\Anaconda3\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\srandrad\\Anaconda3\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\srandrad\\Anaconda3\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\srandrad\\Anaconda3\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      " 19%|███████████████▌                                                                   | 3/16 [00:20<01:26,  6.62s/it]C:\\Users\\srandrad\\Anaconda3\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\srandrad\\Anaconda3\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\srandrad\\Anaconda3\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\srandrad\\Anaconda3\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      " 25%|████████████████████▊                                                              | 4/16 [00:29<01:28,  7.40s/it]C:\\Users\\srandrad\\Anaconda3\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\srandrad\\Anaconda3\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\srandrad\\Anaconda3\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\srandrad\\Anaconda3\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      " 31%|█████████████████████████▉                                                         | 5/16 [00:40<01:32,  8.41s/it]C:\\Users\\srandrad\\Anaconda3\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\srandrad\\Anaconda3\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\srandrad\\Anaconda3\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\srandrad\\Anaconda3\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      " 38%|███████████████████████████████▏                                                   | 6/16 [00:52<01:34,  9.44s/it]C:\\Users\\srandrad\\Anaconda3\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\srandrad\\Anaconda3\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\srandrad\\Anaconda3\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\srandrad\\Anaconda3\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      " 44%|████████████████████████████████████▎                                              | 7/16 [01:06<01:36, 10.76s/it]C:\\Users\\srandrad\\Anaconda3\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\srandrad\\Anaconda3\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\srandrad\\Anaconda3\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\srandrad\\Anaconda3\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      " 50%|█████████████████████████████████████████▌                                         | 8/16 [01:20<01:35, 11.95s/it]C:\\Users\\srandrad\\Anaconda3\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\srandrad\\Anaconda3\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\srandrad\\Anaconda3\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\srandrad\\Anaconda3\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      " 56%|██████████████████████████████████████████████▋                                    | 9/16 [01:36<01:31, 13.11s/it]C:\\Users\\srandrad\\Anaconda3\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\srandrad\\Anaconda3\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\srandrad\\Anaconda3\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\srandrad\\Anaconda3\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      " 62%|███████████████████████████████████████████████████▎                              | 10/16 [01:54<01:26, 14.36s/it]C:\\Users\\srandrad\\Anaconda3\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\srandrad\\Anaconda3\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\srandrad\\Anaconda3\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\srandrad\\Anaconda3\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      " 69%|████████████████████████████████████████████████████████▍                         | 11/16 [02:12<01:17, 15.53s/it]C:\\Users\\srandrad\\Anaconda3\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\srandrad\\Anaconda3\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\srandrad\\Anaconda3\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\srandrad\\Anaconda3\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      " 75%|█████████████████████████████████████████████████████████████▌                    | 12/16 [02:31<01:06, 16.65s/it]C:\\Users\\srandrad\\Anaconda3\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\srandrad\\Anaconda3\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\srandrad\\Anaconda3\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\srandrad\\Anaconda3\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      " 81%|██████████████████████████████████████████████████████████████████▋               | 13/16 [02:51<00:53, 17.76s/it]C:\\Users\\srandrad\\Anaconda3\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\srandrad\\Anaconda3\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\srandrad\\Anaconda3\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\srandrad\\Anaconda3\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      " 88%|███████████████████████████████████████████████████████████████████████▊          | 14/16 [03:15<00:38, 19.45s/it]C:\\Users\\srandrad\\Anaconda3\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\srandrad\\Anaconda3\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\srandrad\\Anaconda3\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\srandrad\\Anaconda3\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      " 94%|████████████████████████████████████████████████████████████████████████████▉     | 15/16 [03:39<00:21, 21.01s/it]C:\\Users\\srandrad\\Anaconda3\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\srandrad\\Anaconda3\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\srandrad\\Anaconda3\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "C:\\Users\\srandrad\\Anaconda3\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 16/16 [04:05<00:00, 15.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_estimators best value: 100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "params = {'booster': ['gbtree', 'gblinear', 'dart'], 'n_estimators': [i for i in range(100, 500, 25)]}\n",
    "tree_params = {'max_depth':[i for i in range(3,25)]}\n",
    "for param in params:\n",
    "    train_r = []\n",
    "    test_r = []\n",
    "    train_MSE = []\n",
    "    test_MSE = []\n",
    "    for val in tqdm(params[param]):\n",
    "        input_params = {param:val}\n",
    "        clf = MultiOutputRegressor(xgb.XGBRegressor(**input_params))\n",
    "        clf.fit(Xtrain, ytrain)\n",
    "        train_r.append(round(clf.score(Xtrain, ytrain),3))\n",
    "        test_r.append(round(clf.score(Xtest, ytest),3))\n",
    "        train_MSE.append(round(mean_squared_error(ytrain, clf.predict(Xtrain)),3))\n",
    "        test_MSE.append(round(mean_squared_error(ytest, clf.predict(Xtest)),3))\n",
    "    print(param+\" best value:\", params[param][np.argmin(test_MSE)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgboost_mdl = xgb.XGBRegressor(booster='gblinear',n_estimators=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'criterion':['mse', 'mae', 'friedman_mse'],\n",
    "         'max_features':['auto', 'sqrt', 'log2'],\n",
    "         'splitter':['best', 'random']}\n",
    "best_params = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/3 [00:00<?, ?it/s]\n",
      "  0%|                                                                                            | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      " 33%|████████████████████████████                                                        | 1/3 [00:00<00:01,  1.94it/s]\u001b[A\n",
      " 67%|████████████████████████████████████████████████████████                            | 2/3 [02:11<00:39, 39.63s/it]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 3/3 [02:11<00:00, 43.96s/it]\u001b[A\n",
      " 33%|███████████████████████████▋                                                       | 1/3 [02:11<04:23, 131.89s/it]\n",
      "  0%|                                                                                            | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      " 33%|███████████████████████████▋                                                       | 1/3 [02:04<04:09, 124.52s/it]\u001b[A\n",
      " 67%|████████████████████████████████████████████████████████                            | 2/3 [02:24<01:33, 93.10s/it]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 3/3 [02:39<00:00, 53.12s/it]\u001b[A\n",
      " 67%|███████████████████████████████████████████████████████▎                           | 2/3 [04:51<02:20, 140.14s/it]\n",
      "  0%|                                                                                            | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████████████████████████                                          | 1/2 [00:23<00:23, 23.96s/it]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:48<00:00, 24.26s/it]\u001b[A\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 3/3 [05:39<00:00, 113.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'mae', 'max_features': 'log2', 'splitter': 'random'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for param in tqdm(params):\n",
    "    train_r = []\n",
    "    test_r = []\n",
    "    train_MSE = []\n",
    "    test_MSE = []\n",
    "    for val in tqdm(params[param]):\n",
    "        test_params = {param:val}\n",
    "        test_params.update(best_params)\n",
    "        clf = MultiOutputRegressor(DecisionTreeRegressor(**test_params))\n",
    "        clf.fit(Xtrain, ytrain)\n",
    "        train_r.append(round(clf.score(Xtrain, ytrain),3))\n",
    "        test_r.append(round(clf.score(Xtest, ytest),3))\n",
    "        train_MSE.append(round(mean_squared_error(ytrain, clf.predict(Xtrain)),3))\n",
    "        test_MSE.append(round(mean_squared_error(ytest, clf.predict(Xtest)),3))\n",
    "    best_params[param] = params[param][np.argmin(test_MSE)]\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AdaBoost Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'base_estimator':[None, DecisionTreeRegressor(criterion='mse', max_features='auto', splitter='random')],\n",
    "          'n_estimators':[i for i in range(50, 550, 50)],\n",
    "          'learning_rate':[1*((10)**(i)) for i in range(-4,3,1)],\n",
    "          'loss':['linear', 'square', 'exponential']\n",
    "         }\n",
    "best_params = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/4 [00:00<?, ?it/s]\n",
      "  0%|                                                                                            | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████████████████████████                                          | 1/2 [00:06<00:06,  6.81s/it]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:20<00:00, 10.10s/it]\u001b[A\n",
      " 25%|█████████████████████                                                               | 1/4 [00:20<01:00, 20.20s/it]\n",
      "  0%|                                                                                           | 0/10 [00:00<?, ?it/s]\u001b[A\n",
      " 10%|████████▎                                                                          | 1/10 [00:13<01:58, 13.18s/it]\u001b[A\n",
      " 20%|████████████████▌                                                                  | 2/10 [00:38<02:13, 16.69s/it]\u001b[A\n",
      " 30%|████████████████████████▉                                                          | 3/10 [01:15<02:39, 22.86s/it]\u001b[A\n",
      " 40%|█████████████████████████████████▏                                                 | 4/10 [02:06<03:08, 31.49s/it]\u001b[A\n",
      " 50%|█████████████████████████████████████████▌                                         | 5/10 [03:10<03:24, 40.98s/it]\u001b[A\n",
      " 60%|█████████████████████████████████████████████████▊                                 | 6/10 [04:22<03:21, 50.48s/it]\u001b[A\n",
      " 70%|██████████████████████████████████████████████████████████                         | 7/10 [05:46<03:01, 60.62s/it]\u001b[A\n",
      " 80%|██████████████████████████████████████████████████████████████████▍                | 8/10 [07:25<02:24, 72.02s/it]\u001b[A\n",
      " 90%|██████████████████████████████████████████████████████████████████████████▋        | 9/10 [09:16<01:23, 83.67s/it]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [11:20<00:00, 68.09s/it]\u001b[A\n",
      " 50%|█████████████████████████████████████████▌                                         | 2/4 [11:41<07:16, 218.42s/it]\n",
      "  0%|                                                                                            | 0/7 [00:00<?, ?it/s]\u001b[A\n",
      " 14%|███████████▊                                                                       | 1/7 [02:22<14:16, 142.72s/it]\u001b[A\n",
      " 29%|███████████████████████▋                                                           | 2/7 [04:39<11:44, 140.80s/it]\u001b[A\n",
      " 43%|███████████████████████████████████▌                                               | 3/7 [06:32<08:50, 132.53s/it]\u001b[A\n",
      " 57%|███████████████████████████████████████████████▍                                   | 4/7 [08:00<05:58, 119.34s/it]\u001b[A\n",
      " 71%|███████████████████████████████████████████████████████████▎                       | 5/7 [09:51<03:53, 116.81s/it]\u001b[A\n",
      " 86%|████████████████████████████████████████████████████████████████████████            | 6/7 [10:06<01:26, 86.20s/it]\u001b[A\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████| 7/7 [10:07<00:00, 86.73s/it]\u001b[A\n",
      " 75%|██████████████████████████████████████████████████████████████▎                    | 3/4 [21:48<05:35, 335.04s/it]\n",
      "  0%|                                                                                            | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      " 33%|███████████████████████████▋                                                       | 1/3 [01:49<03:39, 109.52s/it]\u001b[A\n",
      " 67%|███████████████████████████████████████████████████████▎                           | 2/3 [03:18<01:43, 103.47s/it]\u001b[A\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 3/3 [05:08<00:00, 102.68s/it]\u001b[A\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 4/4 [26:56<00:00, 404.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'base_estimator': DecisionTreeRegressor(max_features='auto', splitter='random'), 'n_estimators': 450, 'learning_rate': 1, 'loss': 'square'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for param in tqdm(params):\n",
    "    train_r = []\n",
    "    test_r = []\n",
    "    train_MSE = []\n",
    "    test_MSE = []\n",
    "    for val in tqdm(params[param]):\n",
    "        test_params = {param:val}\n",
    "        test_params.update(best_params)\n",
    "        clf = MultiOutputRegressor(AdaBoostRegressor(**test_params))\n",
    "        clf.fit(Xtrain, ytrain)\n",
    "        train_r.append(round(clf.score(Xtrain, ytrain),3))\n",
    "        test_r.append(round(clf.score(Xtest, ytest),3))\n",
    "        train_MSE.append(round(mean_squared_error(ytrain, clf.predict(Xtrain)),3))\n",
    "        test_MSE.append(round(mean_squared_error(ytest, clf.predict(Xtest)),3))\n",
    "    best_params[param] = params[param][np.argmin(test_MSE)]\n",
    "print(best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostRegressor(base_estimator=DecisionTreeRegressor(max_features='auto',\n",
       "                                                       splitter='random'),\n",
       "                  learning_rate=1, loss='square')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AdaBoostRegressor(base_estimator=DecisionTreeRegressor(max_features='auto', splitter='random'), n_estimators=50, learning_rate=1, loss='square')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
